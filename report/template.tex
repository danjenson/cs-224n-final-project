\documentclass{article}

\usepackage[final]{neurips_2019}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage[outputdir=build]{minted}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\newcommand{\note}[1]{\textcolor{blue}{{#1}}}
\newcommand{\abs}[1]{\left| #1\right|}
\usepackage{amsmath}
\usepackage{array}

\title{
  Translating Natural Language to Bash Commands using Deep Neural Networks \\
  \vspace{1em}
  \small{\normalfont Stanford CS224N Custom Project}
}

\author{
 Daniel Jenson \\
  Department of Management Science \& Engineering \\
  Stanford University \\
  \texttt{djenson@stanford.edu} \\
  % Examples of more authors
  \And
  Yingxiao Liu \\
  Department of Civil and Environmental Engineering \\
  Stanford University \\
  \texttt{liuyx@stanford.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
  The objective of this project was to generate Bash commands from natural
  language using a deep neural network. We used the NLC2CMD dataset and tested
  3 models: GPT-2, BART, and T5. We also experimented with tokenization methods
  and post-processing to improve accuracy on the competition scoring metric. We
  found that while cross-entropy loss increased steadily for all models,
  only T5 was able to continue learning the structure of Bash commands. After
  post-processing, all models improved, but only T5 and BART exceeded
  the performance of the GPT-3 baseline model.
\end{abstract}


\section{Key Information to include}
\begin{itemize}
	\item TA mentor: Ethan A. Chi
	\item External collaborators: No
	\item External mentor: No
	\item Sharing project: No
\end{itemize}

% {\color{red} This template does not contain the full instruction set for this assignment; please refer back to the milestone instructions PDF.}

\input{introduction_and_related_work}

\section{Approach}
The NLC2CMD Challenge was held once by NeurIPS in 2020. The goal for
competitors was to generate templated commands from natural language commands
that could be used to guide Bash users. Most competitors used GPT-2 as their
base model. This paper also used GPT-2 but also surveyed two additional models,
BART and T5, version 1.1.
\par
The general approach consisted of two principle methods: (1) text generation
and (2) translation. First, it is important to note that Bash is not a
context-free grammar. It admits of very little recursion and, while most
binaries are POSIX compliant, their interfaces are still not entirely
standardized. Flags often carry different semantic meaning and imply
different tasks when employed by different binaries. Moreover, flags often
override, modify, or cancel the intent of other flags in the same command,
introducing complex dependencies. These dependencies can also shift as the
order of the flags and their arguments are permuted. In sum, the meaning of a
flag is almost entirely provided by the invoking binary and its location in the
sequence of arguments. This introduces difficulties in fine-tuning embeddings,
since training may attempt to encode
vastly different meanings in the same embedding. This is particularly
challenging given sparse datasets. Given sufficient training data, it is likely
that that the models may eventually learn correct contextual meaning when
employed by different binaries, but we found 10,000 rows insufficient for the
task. This line of thinking inspired our first approach, text generation using
GPT-2.
\par
While at first this task appears to be a straightforward translation task,
after considering Bash more closely, one can see that it does not admit of many
properties or structures of natural language. Accordingly, we thought that
rather than trying to properly translate natural language into Bash, we could
train a model to hallucinate Bash ``stories'' given natural language. The
high-level idea here is that we fine-tune a GPT-2 model, showing it complete
stories that consist of both a natural language portion and a Bash portion with
some added special tokens. When training, GPT-2 learns common storylines,
and when testing, we feed the trained model only the first half of the
story, i.e. the natural language portion, and ask it to complete the story,
hoping that it will generate Bash commands as the most likely story completion.
In many respects, this idea performs quite well; however, a significant issue
with this approach is constraining responses from GPT-2. How long should the
story be? When does the real content of the ``Bash story'' start and end? What
happens when GPT-2 has multiple endings? These questions are detailed in the
error analysis section.
\par
The second approach we used was a more traditional seq2seq language modeling
approach. Pre-trained models for BART and T5 are easily fine-tuned for
translation tasks. While many natural language modeling tasks admit of a fair
amount of transfer learning because natural languages share some abstract
semantic structures, Bash does not benefit from this nearly as much. As a
non-natural, non-context-free grammar language, modeling it can be difficult,
and our BART model, in particular, struggled with this.

\section{Experiments}
\input{data_and_evaluation_method}

\subsection{Experimental details}
For this task, we tested 3 models: HuggingFace's
\href{https://huggingface.co/GPT-2}{GPT-2}\cite{GPT-2},
\href{https://huggingface.co/facebook/bart-large}{Facebook's BART
	Large}\cite{bart}, and \href{https://huggingface.co/google/t5-v1_1-base}{Google's T5 v1.1
  base}. GPT-2 is a causal model, predicting text from context, while the other
  two are traditional seq2seq models. Each was trained for 5, 10, and 25
  epochs. Batch size was limited to 10 examples to avoid out of memory errors.
  For training, we used the AdamW optimizer with weight decay regularization.
  The learning rate was linear with a warmup of 100 steps. Training time for
  GPT-2, BART, and T5v1.1 was approximately 1, 1.5, and 1.25 hours,
  respectively, on an Azure NC6 instance with a Tesla V100 PCIe 16GB GPU. We
  attempted training the original T5 large model, but even with 5 examples per
  batch, we got out of memory errors; it also took approximately 7 hours to
  fine-tune. All 3 models used cross-entropy loss for training, but were scored
  on the test set using the NLC2CMD metric at the end of each epoch.

\subsection{Results}
While training loss consistently improved, only T5 ultimately began to
learn the structure of Bash commands. Below, you can see that cross-entropy
loss steadily decreased for all 3 models. Curiously, T5 recorded the highest
loss, while performing best on the scoring metric used by the competition.
\begin{center}
	\includegraphics[scale=0.6]{loss.png}
	\includegraphics[scale=0.6]{metric.png}
\end{center}
Despite improving on the training objective, all 3 of these models still
produced garbled and verbose responses. Repetition was common for all 3,
although most with BART. GPT-2 had a tendency to ``ramble;'' it was not uncommon
for GPT-2 to generate natural language intermixed with Bash commands. It also
frequently produced multiple ``target stories.'' Here we define a target story
as a section of output text that begins with the special token
\texttt{<|target|>}, which was used to denote the beginning of a Bash command
in the encoded input. Here is an example row of output from the modeling process with GPT-2:
\begin{minted}{json}
{
  "source": "List the files from the current directory tree that contain
    lines matching regular expression '^From:.*unique sender',
    ignoring ~/src and ~/bin",
  "target": "find Path -name Regex -prune -or -name Regex -prune \
    -or -type f -print | xargs -I {} grep -E -i -l Regex {}",
  "prediction": "<|endoftext|> <|source|> List the files from the current
    directory tree that contain lines matching regular expression
    '^From:.*unique sender', ignoring ~/src and ~/bin <|target|><|endoftext|>
    <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>
    The first day of the year is a long one for the first day of the year.
    <|target|> find Path -name Regex -daystart -type f -print0 \
    | xargs -0 -I {} grep -H Regex {}   "
},
\end{minted}
Here you can see several types of errors affecting performance. A detailed
analysis is left for the subsequent section, but here we will highlight 3
issues that motivated our post-processing. First, you can see that GPT-2 outputs
multiple target stories, i.e. in the prediction, there are two chunks of text
following a special target token. Second, we can see that GPT-2 still had
not internalized the meaning of the target token, because it continued to predict
natural language as well as special end of sentence tokens,
\texttt{<|endoftext|>}, after the target token. Third, we can see hints of
GPT-2's pretraining through the inserted sentence ``The first day of the year is
a long one for the first day of the year.'' This suggests that GPT-2 is still
heavily biased toward its pretraining data, despite being fine-tuned to produce
Bash commands. BART and T5 had similar errors, but those are left for the
analysis section.
\par
Many competitors in the NLC2CMD competition actually crafted extremely
sophisticated post-processing techniques. Some used ensemble models, other
implemented a custom structured beam-search. Given the time limitations for
this project, we elected for a rule-based approach. We ran simple functions
that could be composed across model predictions. We would then score the
prediction after post-processing. We designed 3 post-processing functions to
address our main problems: (1) multiple target texts, (2) repetition or
rambling, and (3) binary prediction.
\par
Looking at the predictions, we noticed that, most often, the target text
closest to a Bash command was the last sequence GPT-2 generated. Accordingly, we
wrote a function we named \texttt{clean} that selected the last chunk of text
associated with a target command. Second, because the scoring metric penalizes
incorrect or excessive flags, we tried to trim repetitions and rambling with a
function called \texttt{max\_len}. Tokenizing by separating on white space, we
collapsed repeated tokens and limited the maximum number of tokens to 15.
Lastly, we attempted to do binary matching. Because the entire prediction's
score hinges largely on selecting the right binary, we wrote a function that
attempted to find the first token in a sequence that closely matched a top 100
Bash utility name. This function only materially improved performance for BART,
but it was extremely noisy can be seen in the above model performance chart.
Using the above prediction and running through \texttt{clean} yields the
following:
\begin{verbatim}
find Path -name Regex -daystart -type f -print0 | xargs -0 -I {} grep -H Regex {}
\end{verbatim}
This is already a significant improvement. Further passing it through
\texttt{max\_len} yields:
\begin{verbatim}
find Path -name Regex -daystart -type f -print0 | xargs -0 -I {} grep -H
\end{verbatim}
While these cleaning techniques are quite crude, they significantly improve
scores. The best model scores under different preprocessing functions along
with the GPT3 baseline are recorded in the following table:

\begin{center}
	\begin{tabular}{rccccc}
		\toprule
    model & raw   & clean & clean+max\_len & binary\\
		\midrule
    GPT-2 & -0.95 & -0.61 & -0.60 & -0.95          \\
    BART  & -0.95 & -0.95 & -0.95 & \textbf{-0.05}         \\
    T5    & -0.13 & -0.06 & \textbf{0.12} & -0.12   \\
		% GPT-2  & -0.951691 & -0.613975 & -0.602972      \\
		% BART  & -0.951691 & -0.951691 & -0.951691      \\
		% T5    & -0.125204 & -0.059434 & 0.119156       \\
		\hline
    GPT3  & -0.19 & -0.19 & -0.19 & -0.19         \\
		\bottomrule
	\end{tabular}
\end{center}

\input{analysis}

\section{Conclusion}
The three main discoveries of this project were: (1) the size of the dataset is
the most important factor in performance, (2) post-processing is required,
especially when training on limited data, and (3) signaling structure to your
model is difficult and subtle but dramatically affects performance, as seen
with BART. While we are satisfied to have surpassed the GPT-3 baseline, the
competition winner achieved a score of 0.53, which still significantly exceeds
the 0.12 achieved by our T5 model. This team, however, augmented their data by
scraping Stack Overflow and similar websites. They also implemented a variety of
additional techniques, which included a custom beam search and ensemble
classification, which were out of the scope of this project. We believe that
success on this task lies principally in cultivating a larger dataset of
parallel translations. T5 was able to achieve notable performance using only
10,000 examples. With 100,000, we are confident that T5 may well exceed the
performance of the top model, even in the absence more sophisticated
techniques.


\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
