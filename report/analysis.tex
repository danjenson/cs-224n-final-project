\section{Analysis}
% TODO: This needs to be cleaned up; I would put all invocations in a table, together
% TODO: What were the primary sources of error for each model? It looks like BART wasn't predicting the binary
We have checked the predictions of all models and listed the primary sources of error for each model in the table below, together with examples.

\newcolumntype{L}{>{\centering\arraybackslash}m{3.7cm}}
\newcolumntype{M}{>{\centering\arraybackslash}m{1.7cm}}
\newcolumntype{N}{>{\centering\arraybackslash}m{8.5cm}}

\begin{center}
\begin{tabular}{ |M|L|L|L| } 
  \hline
  Model & BART & T5 & GPT2 \\ 
  \hline
  Primary sources of error & 
  missing or invalid utilities (like "findfind") & 
  repetition of sequences or redundant identifiers & 
  wrong interpretations of the invocation\\ 
  \hline
  Example target & find Path -name Regex -print | xargs -l -i -I {} wc {} {}, & 
  yes Regex | sed Program &
  sort <( sort -u File ) File File | uniq -u \\ 
  \hline
  Example prediction & 
  Path -name Regex |print0 wargs -I QuantityI -I {} wc - & yes Regex | headt Program yes yes yes yes yes yes & 
  find Path -iname Regex -exec grep -i Regex {}\\
  \hline
\end{tabular}
\end{center}

Because neither BART nor GPT-2 can correctly capture the target utilities, they received a significant penalty for that and their scores were very low. The redundant utilities or identifies, on the other side, did not hurt that much, and consequently T5 got a high score and beat the GPT-3 baseline model.

Another factor affecting the model performance is the data size. The number of examples we have is only 10,000, and we found no other dataset of comparable size on the internet. Our models should behave better if they could learn additional patterns from more training examples.

\begin{center}
\begin{tabular}{|M|N|}
    \hline
     Target&  find Path -path Regex -prune -or -type f -name Regex\\
     \hline
     Prediction (1 epoch)&  findfind Path -name Regex -prune -or -name f -name Regexexecexecexecexecexecexecexecexecexecexecexecexecexecexec\\
     \hline
     Prediction (6 epochs)&  Path -path Regex -prune -or -path f -name Regex -findfindfindfind\\
     \hline
     Prediction (14 epochs)& findfind Path -path Regex -prune -or -name f -name Regex -print - -\\
     \hline
\end{tabular}
\end{center}


Although none of the predictions correctly captured the target command, the model pruned the trailing garbage itself at the end and the predictions became more and more like Bash commands.

Also, another type of error have also been identified for all three models where unnecessary command pipelines were appended in the predictions
\begin{verbatim}
target    : cat File | sort -r -h 
prediction: cat File | sort -n -r | grep -v Regex
\end{verbatim}
indicating that models failed to generate the end of text token at the right position.

Such problems may be alleviated by using more robust models with defined penalty functions. Also, the number of examples we got is only 10,000, and a larger dataset will help to improve the model performance.