\section{Analysis}
The first task in analyzing model performance was error analysis. We checked all predictions on the test set by epoch for each model to discover generalizations, which are summarized in the following table:
\newcolumntype{L}{>{\centering\arraybackslash}m{3.7cm}}
\newcolumntype{M}{>{\centering\arraybackslash}m{1.7cm}}
\newcolumntype{N}{>{\centering\arraybackslash}m{8.5cm}}
\newcolumntype{O}{>{\centering\arraybackslash}m{6cm}}

\begin{center}
	\begin{tabular}{ |M|L|L|L| }
		\hline
		\textbf{Model}                                         & \textbf{BART}                                              & \textbf{T5} & \textbf{GPT-2} \\
		\hline
		Primary sources of error                               &
		missing or invalid binaries (like "findfind")          &
		repetition of sequences or redundant tokens            &
		wrong interpretations of the invocation                                                                                                            \\
		\hline
		Example target                                         & find Path -name Regex -print | xargs -l -i -I {} wc {} {}, &
		yes Regex | sed Program                                &
		sort <( sort -u File ) File File | uniq -u                                                                                                         \\
		\hline
		Example prediction                                     &
		Path -name Regex |print0 wargs -I QuantityI -I {} wc - & yes Regex | headt Program yes yes yes yes yes yes          &
		find Path -iname Regex -exec grep -i Regex {}                                                                                                      \\
		\hline
	\end{tabular}
\end{center}

Because both BART and GPT-2 struggled to correctly capture the target binaries,
they received significant scoring penalties. The redundant binaries or flags
and arguments, on the other hand, were not so harshly penalized; consequently,
T5 was able to edge out the GPT-3 baseline. Curiously, the BART model tended to
get the wrong starting binary for almost all the examples, but did very well in predicting the remainder of the command. Here are some examples of this pathology:
\begin{center}
	\begin{tabular}{ |O|O| }
		\hline
		\textbf{Target}      & \textbf{Prediction} \\
		\hline
		comm -2 -3 File File &
		-2 -3 File File                            \\
		\hline
		chown Regex -R File  &
		own Regex -R File                          \\
		\hline
		mv -f File File      & mmv -f File File    \\
		\hline
	\end{tabular}
\end{center}
Investigating this further, we first confirmed that the model was improving over epochs.
We analyzed a single invocation: ``display all the html files in the current
folder excluding search in the path ./foo'' over several epochs. Although none
of the predictions correctly captured the target binary ``find,'' the model did
improve in pruning trailing repetitions.
\begin{center}
	\begin{tabular}{|M|N|}
		\hline
		Target                 & find Path -path Regex -prune -or -type f -name Regex                                                             \\
		\hline
		Prediction (1 epoch)   & findfind Path -name Regex -prune -or -name f -name Regexexecexecexecexecexecexecexecexecexecexecexecexecexecexec \\
		\hline
		Prediction (6 epochs)  & Path -path Regex -prune -or -path f -name Regex -findfindfindfind                                                \\
		\hline
		Prediction (14 epochs) & findfind Path -path Regex -prune -or -name f -name Regex -print - -                                              \\
		\hline
	\end{tabular}
\end{center}
One possible reason for this aberrant behavior is that BART, while doing a
reasonable job in capturing the intent of the sentence, struggled to develop
individual token accuracy; in particular, the accuracy of the binary token.
Predicting a binary given only the natural language and the
beginning-of-sentence special token was difficult. Because predicting the
binary is so important, we hypothesize that separating BART training into the
following two phases may improve performance: (1) train only on the natural
language command and the singular token corresponding to the correct Bash
utility, and then (2)
fine-tune the model further with the full, templated Bash commands. Improving
binary prediction in BART would likely make it competitive with T5.
\par
We also investigated GPT-2 prediction errors. Here is an example prediction for the natural language invocation: ``Change the ownership of all files in the current directory tree from root to www-data'':

\begin{verbatim}
<|endoftext|> <|source|> Change the ownership of all files in the current 
directory tree from root to www-data <|target|> (omit 23 <|endoftext|> tokens 
here) Synchronize file systems to /tmp/ and output the result to console 
<|target|> df File | awk Program | xargs -I {} ls -a -l -d -S -r File
\end{verbatim}
The model incorrectly generated another invocation: ``Synchronize file systems to /tmp/ and output the result to console.'' This has greater implications than simply generating additional cruft to be trimmed; it actually corrupts the hidden state of the model. Consequently, the final prediction was orthogonal to the target command \texttt{find Path -user Regex -exec chown Regex \{\}}.
\par
The last error class, which was demonstrated by all 3 models, was ``rambling,'' or inserting additional command sequences after the target sequence. Here is an example:
\begin{verbatim}
target    : cat File | sort -r -h 
prediction: cat File | sort -n -r | grep -v Regex
\end{verbatim}
This suggests that all the models failed to generate the end-of-text token in
the correct location.
\par
While all of these models could be improved with longer training, phased
training, and more sophisticated post-processing, the principal factor
affecting performance was data size. 10,000 examples is insufficient to
generate very accurate translations, and we were unable to find additional data
sources that did not require significant preprocessing.
