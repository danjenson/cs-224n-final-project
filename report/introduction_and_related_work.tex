\section{Introduction}
Bash or the Bourne Again Shell, is a standard and popular command line
interface to Unix-based computer systems. Despite its popularity, it has a
very steep learning curve. Novitiates are often overwhelmed by the concepts of
binaries, flags, and arguments. Even experienced engineers frequently consult
man-pages, online documentation, and online forums like Stack Overflow to learn
about the particulars of various commands. This project aims to ease those
burdens on new and experienced users alike, and to develop tools to generate Bash
commands from natural language. We want to provide a natural language
interface that enables people to interact with computers through natural
languages, and thus making programming resources more accessible to the
general public.
\par
However, translating natural language into Bash can be challenging; many
natural language queries or commands can be converted into the same Bash
command. Conversely, many Bash commands may correspond to the same natural
language command, due to English's inherent ambiguity and the required specificity
of Bash. Thus, there is a many-to-many relationship between natural
language and Bash commands. Further compounding this difficulty is that Bash
commands can be composed, generating pipelines of commands corresponding to
entire data flows. Lastly, the meaning of these commands all shifts when
either the order of the commands or their arguments are permuted. For this
reason, generating a perfectly correct Bash command from natural language can
be extremely complex.
\par
In order to tackle some of these problems, we used the data from the NeurIPS
2020 NLC2CMD Challenge, and experimented with several transformer models,
including GPT-2, BART, and T5, as well as different tokenization and
post-processing schemes. We evaluated the model performance in terms of both
the training loss and a specific metric measuring the accuracy of the
prediction, and compared our models with the baseline model provided by the
competition.

\section{Related Work}
Code generation is a variant of semantic parsing, and a significant amount of
research has been published in this area. One of the earliest and most
successful studies was conducted to translate natural language to SQL
queries. Zhong et al. (2017) \cite{zhong2017seq2sql} proposed a deep
augmented pointer network and a loss function supplemented by reinforcement
learning. In the SQL domain, they were able to achieve an execution accuracy of
60\%. Notably, however, SQL has a singular, well-defined syntax with a
context-free grammar. Accordingly, this model does not always generalize well
to programming languages like Bash.
\par
For high-level programming language generation, there are a number of recent
attempts to translate well-structured natural language input into Java or
Python.  Ling et al. (2016) \cite{ling2016latent} proposed a generative model
with a multiple pointer network to generate code from texts in Trading Card
Games, although the selected input language in the games is very well-defined. A
more robust syntax-based model was developed by Yin and Neubig (2017)
\cite{yin2017syntactic} and tested on the same dataset, but performance did not
increase materially. Rahit et al.(2019) \cite{rahit2019machine} used recurrent
neural network (RNN) and long-short term memory (LSTM) cells to build their model and
reported an accuracy as high as 74\% when the input was prepared in a format
closer to pseudocode with keywords such as ``define'' and ``if-else.''
\par
In the specific domain of Bash command generation, Lin et al. (2018)
\cite{lin2018nl2bash} modified the seq2seq model by adding gated recurrent
units (GRU) and RNN cells and introducing a copying mechanism. The model was
evaluated manually by people, rather than by an objective metric, and the
accuracy was reported to be 0.29. Fu et al. (2021) \cite{Fu2021ATransform}
built a transformer model combined with a custom beam search and won the NLC2CMD
Challenge competition. They tested different models and concluded that
transformer-based models could significantly outperform the RNN-based models.
